---
title: "General workflow to collate, format, data enrichment and check"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{General workflow to collate, format, data enrichment and check}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---


```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup, echo=FALSE, message=FALSE, warning=FALSE}
library(danubeoccurR)
library(hydrographr)
library(specleanr)
library(dplyr)
```

## Introduction

This vignette demonstrates how to use the functions in the danubeoccurR package
to download and clean fish species occurrence records from the Global 
Biodiversity Information Facility (GBIF) for the Danube River Basin. It covers
the steps to filter, download, and clean the data to prepare it for further
analysis. 

Before starting, ensure that you have:

- An active internet connection to access GBIF data.
- A GBIF API key.

Additionally, ensure that you follow the <a href="https://glowabio.github.io/hydrographr/articles/hydrographr.html" target="_blank">installation guideline</a> of the package `hydrographr`  for your operating system.


    
## Step 1: Install the Package from GitHub

To get started, you need to install the danubeoccurR, hydrographr and specleanr
packages from GitHub. You can do this using the devtools package:

```{r, eval=FALSE}
# Install the devtools package if you haven't already
install.packages("devtools")

# Install the danubeoccurR package from GitHub
devtools::install_github("ytorres-cambas/danubeoccurR")
devtools::install_github("glowabio/hydrographr")
devtools::install_github("AnthonyBasooma/specleanr")
library(danubeoccurR)
library(hydrographr)
library(specleanr)
library(dplyr)
library(sf)
```

## Step 2: Downloading GBIF Data

After installing the package, you can download fish species occurrences using 
the `download_gbif_records()` function. This will fetch records that fall within
the geographic boundary of the Danube River Basin.

```{r,eval=TRUE}
# Define species to download. We will use only three species from a check list
# of fish species form the Danube River Basin. The complete check list is data
# set include with the package danubeoccurR
species_list <- species_checklist[1:2]
print(species_list)
```

```{r,eval=TRUE}
# Define a geographic bounding box to use for querying GBIF occurrences within
# the specified area. The bounding box should be a simple feature (sf) object, 
# typically imported into R with sf::read_sf().
# In this example, we use an sf object that is included in the danubeoccurR 
# package, so there is no need to import it manually.
bbox_danube_basin
```

```{r,eval=TRUE}
# Convert the polygon to WKT format using the `get_wkt` function. 
# This format is required for GBIF. The function takes an sf polygon object, 
# ensures that the polygon has a counterclockwise winding order, 
# and returns the WKT (Well-Known Text) representation of the polygon.
wkt <- get_wkt(bbox_danube_basin)
print(wkt)
```

This function retrieves fish species occurrences from GBIF based on the species
list and the polygon that represents the Danube River Basin. 
We set `import_to_r = TRUE` to import the occurrences into R. The function 
returns a list with a data frame with occurrences and data for citation.

```{r,eval=FALSE}
# Get the path to the temporary directory to save occurrences
temp_path <- tempdir() 

# Download occurrences from GBIF
gbif_data <- download_gbif_records(species_names = species_list,
                                   wkt = wkt,
                                   output_occur_path = temp_path,
                                   gbif_user = "your_username",
                                   gbif_pwd = "your_password",
                                   gbif_email="your_email@example.com",
                                   import_to_r = TRUE)
```

## Step 3: Cleaning GBIF Data

Once the occurrences are downloaded, we will clean the data using `clean_gbif()`.
This function removes duplicates, invalid coordinates, and other quality issues.
This ensures that the data is ready for analysis, without missing or erroneous 
records.

```{r, eval=TRUE, echo=TRUE}
# Clean the downloaded GBIF records
gbif_cleaned <- clean_gbif(gbif_data$raw_download,
                                  coordinateUncertaintyInMeters = 1000,
                                  coordinatePrecision = 0.01,
                                  buffer_centroid_zoo_herbaria = 1000,
                                  buffer_centroid_capitals = 1000,
                                  buffer_centroid_countries = 1000)
```

As we can see, `r nrow(gbif_cleaned)` out of `r nrow(gbif_data$raw_download)` records were 
retained after the cleaning procedure.

```{r, eval=TRUE, echo=TRUE}
# Number of records before cleaning
nrow(gbif_data$raw_download)

# Number of records after cleaning
nrow(gbif_cleaned)

# Preview the cleaned data
head(gbif_cleaned)
```

## Step 4: Visualizing the Occurrences

We can now visualize the occurrences on a map using `visualize_points()`. 

```{r, eval=TRUE, echo=TRUE, message=FALSE, fig.width=7, fig.height=5}
visualize_points(points_df = gbif_cleaned,
                 layer = danube_basin)
```

As shown, some occurrence records fall outside the Danube River Basin. This happens because we used a bounding box as the spatial reference when querying GBIF. To limit the data strictly to the Danube River Basin, we can apply spatial filtering using the `get_spatial_subset()` function. This function filters the points based on a layer representing the Danube River Basin, which is included as one of the datasets in the `danubeoccurR` package.

```{r, eval=TRUE, echo=TRUE, message=FALSE}
# Spatial filtering of occurrence records
gbif_filtered <- get_spatial_subset(danube_basin,
                                    gbif_cleaned,
                                    "decimalLatitude",
                                    "decimalLongitude",
                                     verbose = TRUE)
```

Now, the map displays only the occurrence records that are strictly located within the Danube River Basin.

```{r, eval=TRUE, echo=TRUE, message=FALSE, fig.width=7, fig.height=5}
visualize_points(points_df = gbif_filtered,
                 layer = danube_basin)
```


## Step 6: Checking species names

To clean the species names and ensure consistency with FishBase, we will use the `check_names()` function from the package `specleanr`. This function matches species names to FishBase and, by default, returns the accepted name if a synonym is provided. The function also checks for spelling errors and suggests the closest match in FishBase based on the similarity threshold defined by the pct parameter. We will use the default pct value of 90 that indicates a match of 90% or above is required.

```{r, eval=TRUE}
# Check species names for inconsistencies 
gbif_name_checked <- check_names(data = gbif_filtered,
                                 colsp = "species",
                                 verbose = FALSE,
                                 pct = 90,
                                 merge = F,
                                 sn = FALSE)

# Preview the species names
print(gbif_name_checked)
```

As you can see, the species names in the `species` column, which correspond to the species names in our dataset, match the names in the `speciescheck` column from FishBase, indicating that no changes are required.

## Step 7: Add additional taxonomic information

If taxonomic information for Genus, Family, or Order is missing from some occurrences, it can be retrieved from FishBase and added. While the example dataset does not have missing taxonomic information, its classification differs from that used in FishBase. Therefore, we will update the taxonomic classification in the example dataset based on the information retrieved from FishBase. We will use the species name list obtained in the previous step after verifying species names against FishBase.

```{r, eval=TRUE}
# Return genus, family and order of the species in the input vector. 
tax_info <- get_taxonomy_info(species = gbif_name_checked$speciescheck, sources = "fishbase")

# Visualize result
print(tax_info)

# Visualize taxonomic classification used in the example dataset.
gbif_filtered %>%
  select(species, genus, family, order) %>%
  distinct(species, .keep_all = TRUE) %>%
  print()
```

The result is a data frame containing columns for Species, Genus, Family, and Order. In this case, the species **Abramis brama** is classified under the Family Leuciscidae; however, it was classified under Cyprinidae in the example dataset downloaded from GBIF. We will now proceed to update the Family classification of **Abramis brama**.

```{r, eval=TRUE}
# Update taxonomic information
gbif_filtered[gbif_filtered$family=="Cyprinidae", "family"] <- "Leuciscidae"
```

## Step 8: Add administrative information
Administrative information, such as country name and lower administrative levels (e.g., administrative levels 1 and 2 from the Database of Global Administrative Areas), can be added by performing a spatial join between points and GADM Administrative Boundaries using the `spatial_join_gadm()` function. If a vector layer is not supplied but an ISO3 country code is provided, the function attempts to download GPKG files from GADM and use them for the spatial join. If the download fails due to a slow connection, the `timeout` argument value can be increased to allow more time for the process. As a final step, the columns are renamed to align with [Darwin Core](https://dwc.tdwg.org/) standards. In this case, GADM administrative levels 1 and 2 correspond to the [Darwin Core](https://dwc.tdwg.org/) terms `stateProvince` and `county`, respectively. Therefore, we will rename the columns accordingly.

```{r, eval=TRUE, echo=FALSE, message=FALSE, warning=FALSE}

# Download information from Database of Global Administrative Areas for each country in the Danube River Basin
# ISO3 country codes
country_codes = c("ALB", "AUT", "BIH", "BGR", "CHE", "CZE", "DEU", "HUN","ITA", "HRV",
                      "MDA", "MKD","MNE", "POL", "ROU", "SRB", "SVK", "SVN", "UKR", "XKO")

## Folder to save temporal files
temp <-  paste0(getwd(), "/temp/")
if(!dir.exists(temp)) dir.create(temp)

# Convert input data frame to an sf object
  points_sf <- st_as_sf(
    gbif_filtered,
    coords = c("decimalLongitude", "decimalLatitude"),
    crs = 4326
  )

      # Initialize a list to store administrative sf objects
    admin_sf_list <- list()

    tryCatch({
      for (country_code in country_codes) {
        # gadm_url <- paste0(
        #   "https://geodata.ucdavis.edu/gadm/gadm4.1/gpkg/gadm41_",
        #   country_code,
        #   ".gpkg"
        # )
        gadm_file <- gsub("\\\\", "/", file.path(temp, paste0("gadm41_", country_code, ".gpkg")))
        # download.file(gadm_url, gadm_file, mode = "wb")
        # message(paste("Downloaded GADM file for", country_code))

        layers <- st_layers(gadm_file)$name
        layer <- if ("ADM_ADM_2" %in% layers) {
          st_read(gadm_file, layer = "ADM_ADM_2", quiet = TRUE)
        } else if ("ADM_ADM_1" %in% layers) {
          st_read(gadm_file, layer = "ADM_ADM_1", quiet = TRUE)
        } else if ("ADM_ADM_0" %in% layers) {
          st_read(gadm_file, layer = "ADM_ADM_0", quiet = TRUE)
        } else {
          warning(paste("No suitable administrative layer found for", country_code))
          NULL
        }

        if (!is.null(layer)) {
          admin_sf_list[[country_code]] <- layer
        }
      }

      if (length(admin_sf_list) == 0) {
        stop("No valid administrative layers found for any of the countries.")
      }
      
      # Merge administrative layers
      merged_admin_sf <- do.call(bind_rows, admin_sf_list)
      
      # Join occurrence points and administrative layers
      joined_sf <- st_join(points_sf, merged_admin_sf, join = st_within)
      
    }, finally = {
      unlink(temp_dir, recursive = TRUE)
      message("Temporary files deleted.")
    })
    
    # Convert back to data frame
  result <- joined_sf %>%
    st_drop_geometry() %>%
    mutate(
      decimalLongitude = st_coordinates(points_sf)[, "X"],
      decimalLatitude = st_coordinates(points_sf)[, "Y"]
    )
  # Rename columns
  admin_4_gbif  <- result %>%
    select(-stateProvince) %>%
    rename(country = COUNTRY,
           county = NAME_2,
           stateProvince = NAME_1)
``` 

```{r, eval=FALSE}
# Perform a spatial join of points with GADM Administrative Boundaries 
  admin_4_gbif <-  spatial_join_gadm(
  coords_df = gbif_filtered,
  country_codes = c("ALB", "AUT", "BIH", "BGR", "CHE", "CZE", "DEU", "HUN","ITA", "HRV",
                      "MDA", "MKD","MNE", "POL", "ROU", "SRB", "SVK", "SVN", "UKR", "XKO"),
  lat_col = "decimalLatitude",
  lon_col = "decimalLongitude",
  timeout = 1000)
# Rename columns
admin_4_gbif <- admin_4_gbif %>%
    select(-stateProvince) %>% # delete original column
    rename(country = COUNTRY,
           county = NAME_2,
           stateProvince = NAME_1)

```

## Step 9: Extracting sub-catchment ids

Next, we will extract the IDs of the sub-catchments where each occurrence is located and add a new column with these IDs to the dataset. To achieve this, we will use the `extract_ids()` function from the <a href="https://github.com/glowabio/hydrographr" target="_blank">hydrographr package</a>. The sub-catchment layer was generated from the <a href="https://essd.copernicus.org/articles/14/4525/2022/essd-14-4525-2022.html" target="_blank">Hydrography90m dataset</a> by downloading the tiles corresponding to the Danube River Basin and cropping them using functions from the <a href="https://github.com/glowabio/hydrographr" target="_blank">hydrographr package</a>. 

Lets first download, crop and merge raster tiles with sub-catchments.


```{r, eval=FALSE}

# Folder to save final Hydrography90m layers
# Create output folder if it doesn't exist
output_hydro90m <- paste0(getwd(), "/hydrography90m")
if(!dir.exists(output_hydro90m)) dir.create(output_hydro90m)

# Folder to save temporal files
temp <-  paste0(getwd(), "/temp")
if(!dir.exists(temp)) dir.create(temp)

# Define tile ID
id <- c("h18v02", "h18v04", "h20v02", "h20v04")

# Define raster variable
r_var <- c("sub_catchment")

# raster files
download_tiles(variable = r_var,
               file_format = "tif",
               tile_id = id,
               download_dir = temp)


# Get the full paths of the raster tiles
raster_dir <- list.files(paste0(temp, "/r.watershed"), pattern = ".tif",
                         full.names = TRUE, recursive = TRUE)

# Crop raster tiles to the extent of a sf object
for(itile in raster_dir) {

  crop_to_extent(raster_layer = itile,
                 bounding_box = bbox_danube_basin,
                 out_dir = temp,
                 file_name =  paste0(str_remove(basename(itile), ".tif"),
                                     "_tmp.tif"))

}

# Merge filtered raster tiles and save result to a directory
merge_tiles(tile_dir = temp,
            tile_names = list.files(temp,
                                    full.names = FALSE,
                                    pattern = "_tmp.tif"),
            out_dir = output_hydro90m,
            file_name = "subcatch_danube.tif",
            name = "ID",
            read = FALSE,
            compression = "high",
            quiet = FALSE)
```

Now, we are ready to extract the sub-catchment IDs.

```{r, eval=FALSE}
# Create unique ids for rows in gbif_filtered. It is required by the function
# extract_ids. It will be removed later
admin_4_gbif$id_col <- seq(1:nrow(admin_4_gbif))

# Extract sub-catchment IDs 
ids_df <- extract_ids(
    data = admin_4_gbif,
    lon = "decimalLongitude",
    lat = "decimalLatitude",
    id = "id_col",
    basin_layer = NULL,
    subc_layer = paste0(getwd(), "/hydrography90m/subcatch_danube.tif"),
    quiet = FALSE
  ) 

# Check all occurrences have sub-catchment IDs
if(length(which(is.na(ids_df$subcatchment_id))) == 0) {
  print("All occurrences have sub-catchment IDs.")
} else {
  print(paste0(length(which(is.na(ids_df$subcatchment_id))),
               " occurrences are missing sub-catchment IDs."))
}

# New column with sub-catchment IDs
gbif_subcID <- admin_4_gbif %>%
  mutate(subcatchmentID = ids_df$subcatchment_id) %>%
  select(-id_col) # delete column with row ids 

```

# Step 10 Add occurrence IDs 
To ensure each record has a unique and persistent identifier, we assigned Universal Unique Identifiers (UUIDs) using the `generate_global_identifier()` function. This aligns with Darwin Core standards and facilitates data integration across biodiversity databases.

First, we identified records that lacked an `occurrenceID`. For these records, we generated and assigned a UUID using the `generate_global_identifier()` function. This step ensures that all occurrences have a globally unique identifier, improving traceability and interoperability with other datasets.

```{r, eval=FALSE}
# Identify records that lack an occurrenceID
indx <- which(gbif_subcID$occurrenceID=="")

# Generate occurrence IDs
ids <- generate_global_identifier(n = length(indx))

# Add ids
gbif_subcID[indx,"occurrenceID"] <- ids
```

## Step 11: Detecting Environmental Outliers

In the context of Species Distribution Models (SDMs), **environmental outliers** refer to species occurrence records that are associated with extreme or unusual environmental conditions, diverging significantly from the majority of other occurrence points. These outliers may arise for several reasons, including:

- **Measurement Errors**: Inaccuracies in environmental data (e.g., temperature, precipitation) recorded at a specific location, leading to false readings.
  
- **Spatial Errors**: Misplacement of occurrence points, which results in a mismatch between species records and their true environmental conditions.
  
- **Natural Anomalies**: Rare occurrences of species in unusual or extreme environments, which may be biologically significant but differ from the typical conditions where most occurrences are found.

Environmental outliers can impact SDMs by distorting the relationships between species occurrences and environmental variables. This may result in:

- **Overfitting**: The model may overemphasize patterns specific to the outlier points, misrepresenting the broader ecological niche of the species.
  
- **Erroneous Predictions**: The model might incorrectly identify unsuitable areas as habitable, based on the outlier values.

To address this, we can use the `multidetect()` function from the [spcleanr](https://github.com/AnthonyBasooma/specleanr) package to detect environmental outliers. Once identified, a remark is added in the `occurrenceRemarks` field, indicating that the record is a potential environmental outlier.


```{r, eval=FALSE}
gbif_subcID
gbif_det_outlier <- 
```

## Step 12 Check data format

Finally, to ensure data consistency and compliance with Darwin Core standards, we will perform a series of format checks.

* Column names were verified using the `check_column_name()` function to confirm they conform to Darwin Core terms as well as additional custom names.

* Year values were checked for missing entries and validated to be numeric and within an acceptable range using the `check_year_column()` function. The function output is a list of:
  ** `missing_values`: Indices of missing values in the year column.
  ** `invalid_years`: Indices of values that fall outside the valid year range.
  ** `updated_df`: A data frame with the updated year values and a flag indicating

* Coordinate validation was conducted using the `check_coordinates()` function to ensure latitude and longitude values follow the correct decimal degree format (WGS84).

These checks help maintain data quality and facilitate seamless integration with biodiversity databases.

```{r, eval=FALSE}
# Check column name
check_column_name(df_input = ,
                  standard_terms = c(dwc_names, "waterBodyType", "gbifID", "subcatchmentID"),
                               verbose = TRUE)

# Check for missing values, ensure that values are numeric and fall within a valid range. The result is a list with three elements that store "missing_values",
gbif_year_checked <- check_year_column(df = gbif_det_outlier,
                  col_year = "year",
                  year_range = c(1800, 2024))
print(gbif_year_checked$missing_values) 
print(gbif_year_checked$invalid_years)

# Check coordinates format and ensure they are numeric data type
gbif_coord_checked <- check_coordinates(df = gbif_year_checked$updated_df,
                                        lat_col = "decimalLatitude",
                                        lon_col = "decimalLongitude",
                                        convert_to_numeric = TRUE,
                                        verbose = TRUE)

```


## Conclusion

This vignette shows how to download and clean fish species occurrence data from
GBIF for the Danube River Basin using the danubeoccurR package. The cleaned data
can be further analyzed or used for conservation strategies. This pipeline can 
also be integrated into broader biodiversity assessment workflows.

## Acknowledgements

Funding was provided through the DANUBE4all project, funded by the European 
Unionâ€™s Horizon Europe research and innovation programme under grant 
agreement no. 101093985.


